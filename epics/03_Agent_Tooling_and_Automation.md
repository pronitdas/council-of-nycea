# Epic 3: Agent Tooling and Automation Capabilities

## Description

This epic transforms agents from passive discussants into active participants capable of interacting with external systems and performing automated tasks. It involves equipping agents with the ability to use "tools" – predefined functions or API calls – to achieve specific goals. This could range from simple actions like fetching current data from an API (e.g., stock prices, weather) to complex operations like interacting with code repositories (reading files, suggesting changes), executing code snippets in a sandboxed environment, calling specialized APIs (like the available Maps tools), creating or updating JIRA tickets, or even generating draft documents (PRDs, test plans). The core is implementing a robust function calling mechanism (compatible with models like OpenAI's function calling, Gemini's tool use, or open-source equivalents like Gorilla LLM) within the agent's reasoning loop.

## User Stories

- **As an Agent,** I want to identify when a task requires external action or information beyond my internal knowledge and the provided context, select the appropriate tool, formulate the correct input parameters, and call the tool, so I can fulfill requests or contribute more effectively (e.g., "Fetch the latest status of JIRA-123", "Generate Python code for a function to calculate Fibonacci numbers", "Find cafes near 12.93,77.61").
- **As a Developer defining an Agent's Persona,** I want to easily define and register available tools (functions, API calls) with clear descriptions, parameters, and expected outputs, so agents can understand when and how to use them.
- **As a Discussion Participant,** I want to see a clear indication when an agent is using a tool and what the outcome was (success, failure, data retrieved), so I can understand the agent's actions and trust its outputs.
- **As a System Administrator,** I want control over which tools specific agents or agent types are allowed to use, and implement safety measures (like sandboxing for code execution or rate limiting for external APIs), so I can ensure security and manage costs.

## Potential Pitfalls

- **Tool Selection Ambiguity:** The LLM might struggle to choose the correct tool among several similar options or fail to recognize when a tool is needed.
- **Parameter Hallucination:** The LLM might generate incorrect or nonsensical parameters for the tool call.
- **Error Handling for Tool Execution:** Tools can fail for various reasons (API downtime, invalid input, permission errors, code execution errors). The agent needs robust mechanisms to handle these failures and potentially retry or report the issue.
- **Security Risks:** Allowing agents to execute code or interact with external systems inherently introduces security risks. Requires careful sandboxing, input validation, and permission management.
- **Tool Definition Brittleness:** Poorly defined tool descriptions or parameters can confuse the LLM. Tool schemas need to be clear and robust.
- **Latency:** Tool execution (especially code execution or complex API calls) can add significant latency to the agent's response time.
- **Cost Management:** Frequent use of external APIs or computationally intensive tools can lead to unexpected costs.

## Good Practices

- **Clear Tool Definitions:** Use a structured format (like JSON Schema) to define tools, including clear descriptions of functionality, parameters (with types and descriptions), and expected return values. Provide examples if possible.
- **Few-Shot Prompting for Tool Use:** Include examples of correct tool usage (thought process, tool selection, parameter generation) in the agent's base prompt to guide the LLM.
- **Input Validation:** Rigorously validate parameters generated by the LLM before passing them to the actual tool execution logic.
- **Sandboxing:** Execute potentially risky tools (like code execution) in isolated, secure environments (e.g., Docker containers, WebAssembly runtimes) with strict resource limits.
- **Error Handling and Retries:** Implement strategies for handling tool failures, potentially including retries with backoff, or prompting the LLM to try a different approach.
- **Tool Orchestration Framework:** Use libraries or frameworks designed for LLM tool use (e.g., LangChain agents, LlamaIndex tools, Haystack agents) to simplify implementation and benefit from built-in error handling and reasoning loops.
- **User Feedback Loop:** Provide mechanisms for users to give feedback on incorrect or problematic tool usage to help fine-tune the system.
- **Monitoring and Rate Limiting:** Monitor tool usage frequency and success/failure rates. Implement rate limiting for costly or sensitive tools.
- **Multi-turn Tool Use:** Design the system to handle scenarios where multiple tool calls might be needed to accomplish a single task.

## Definition of Done (DoD)

- Agent architecture supports invoking external functions/tools based on LLM decisions.
- At least two distinct tools are implemented and integrated (e.g., a simple API call like fetching weather, and a more complex one like semantic code search via the Knowledge Graph API from Epic 2).
- Agents can successfully select and call the implemented tools with correctly formatted parameters in relevant scenarios (demonstrated via integration tests).
- Tool execution failures are handled gracefully (e.g., agent reports the failure or tries an alternative).
- Tool definitions are documented clearly.
- Basic security measures are in place for the implemented tools (e.g., input validation, API key management if applicable).
- Frontend UI indicates when an agent is using a tool and displays the result (or failure).

## End-to-End (E2E) Flows

1.  **Agent Uses a Knowledge Search Tool:**
    - Discussion progresses, agent identifies a need for specific information not in the immediate context.
    - LLM reasoning process (prompt includes tool definitions) determines the `knowledge_search` tool is appropriate.
    - LLM generates the required parameters (e.g., `query: "details on auth bug JIRA-456"`, `filter: {source: 'jira'}`).
    - Agent framework intercepts the tool call request.
    - Framework validates parameters against the tool's schema.
    - Framework executes the tool: Calls the Knowledge Search API (`POST /api/v1/knowledge/search`).
    - Knowledge Search API returns results (or an error).
    - Framework passes the results (or error message) back to the LLM as tool output.
    - LLM incorporates the tool output into its next reasoning step and generates the final response.
    - Agent sends the final response (potentially mentioning the search results) to the Discussion Service.

2.  **Agent Uses a Code Execution Tool (Simplified):**
    - Agent is asked to "calculate the sum of squares for numbers 1 to 5".
    - LLM reasoning identifies the need for computation and selects the `python_interpreter` tool.
    - LLM generates parameters: `code: "sum([x*x for x in range(1, 6)])"`.
    - Agent framework intercepts the tool call.
    - Framework validates parameters.
    - Framework sends the code to a sandboxed Python execution environment (e.g., a secure API endpoint or container).
    - Sandbox executes the code and captures the result (`stdout`/`stderr` or return value).
    - Sandbox returns the result (e.g., `"55"`) or error information to the framework.
    - Framework passes the result/error back to the LLM.
    - LLM generates the final response: "The sum of squares for numbers 1 to 5 is 55."
    - Agent sends the response to the Discussion Service. 


# Epic 6: Agent Tool Use Framework and Integration

## Description

This epic establishes a comprehensive tool use framework that transforms the Council of Nycea agents from conversational participants into autonomous actors capable of interacting with external systems, APIs, and services. Building on the existing agent architecture defined in `agent.ts`, this epic introduces tool definitions, execution engines, security frameworks, and UI components to support sophisticated agentic workflows. The framework will support both synchronous tools (immediate API calls, calculations) and asynchronous tools (long-running processes, multi-step workflows), with robust error handling, permission management, and observability. This positions the platform for true agentic startup capabilities where agents can perform real work beyond discussion.

## User Stories

- **As an Agent,** I want to access a curated toolkit of functions (API calls, code execution, file operations, web searches, database queries) that I can invoke when my reasoning determines external action is needed, so I can provide actionable solutions rather than just discussion.

- **As a Persona Designer,** I want to define tool permissions and preferences for each agent persona (e.g., a DevOps agent has access to deployment tools, a Research agent has access to academic databases), so agents behave consistently with their defined roles and expertise.

- **As a Discussion Moderator,** I want to see real-time tool usage indicators, execution logs, and results in the discussion interface, so I can understand what actions agents are taking and validate their outputs.

- **As a System Administrator,** I want granular control over tool access, execution limits, cost monitoring, and security policies, so I can safely enable powerful capabilities while managing risks and expenses.

- **As a Developer,** I want a standardized tool definition format and registration system that makes it easy to add new capabilities to agents, so the platform can rapidly expand its agentic capabilities.

- **As an End User,** I want agents to seamlessly transition between discussion and action, clearly communicating their intentions and results, so I can trust and collaborate with them effectively.

## Enhanced Agent Types

### Extended AgentState Interface
```typescript
export interface AgentState {
  // ... existing properties ...
  
  // Tool-related properties
  availableTools: string[];
  toolPermissions: ToolPermissionSet;
  toolUsageHistory: ToolUsageRecord[];
  currentToolExecution?: ToolExecution;
  toolPreferences: ToolPreferences;
  maxConcurrentTools: number;
  toolBudget?: ToolBudget;
}

export interface ToolDefinition {
  id: string;
  name: string;
  description: string;
  category: ToolCategory;
  parameters: JSONSchema;
  returnType: JSONSchema;
  examples: ToolExample[];
  securityLevel: 'safe' | 'moderate' | 'restricted' | 'dangerous';
  costEstimate?: number;
  executionTimeEstimate?: number;
  requiresApproval: boolean;
  dependencies: string[];
}

export interface ToolExecution {
  id: string;
  toolId: string;
  agentId: string;
  parameters: Record<string, any>;
  status: 'pending' | 'running' | 'completed' | 'failed' | 'cancelled';
  startTime: Date;
  endTime?: Date;
  result?: any;
  error?: string;
  approvalRequired: boolean;
  approvedBy?: string;
  cost?: number;
}

export interface ToolUsageRecord {
  toolId: string;
  timestamp: Date;
  success: boolean;
  executionTime: number;
  cost?: number;
  errorType?: string;
}

export type ToolCategory = 
  | 'api' | 'computation' | 'file-system' | 'database' 
  | 'web-search' | 'code-execution' | 'communication' 
  | 'knowledge-graph' | 'deployment' | 'monitoring';
```

### Enhanced Message Types
```typescript
export interface Message {
  // ... existing properties ...
  
  // Tool-related properties
  toolCalls?: ToolCall[];
  toolResults?: ToolResult[];
  requiresToolApproval?: boolean;
  toolExecutionSummary?: string;
}

export interface ToolCall {
  id: string;
  toolId: string;
  parameters: Record<string, any>;
  reasoning: string;
  confidence: number;
}

export interface ToolResult {
  callId: string;
  success: boolean;
  result?: any;
  error?: string;
  executionTime: number;
  cost?: number;
}
```

## Core Tool Categories

### 1. Knowledge & Research Tools
- **Semantic Search**: Query the knowledge graph from Epic 2
- **Web Search**: Real-time web search with result summarization
- **Document Analysis**: Parse and analyze uploaded documents
- **Academic Search**: Query research databases and papers

### 2. Development & DevOps Tools
- **Code Execution**: Sandboxed Python/JavaScript/SQL execution
- **Git Operations**: Repository cloning, file reading, commit analysis
- **API Testing**: HTTP request testing and validation
- **Deployment**: Integration with CI/CD pipelines
- **Monitoring**: Query metrics and logs from observability systems

### 3. Communication & Collaboration Tools
- **Email/Slack**: Send notifications and updates
- **Ticket Management**: Create/update JIRA, GitHub issues
- **Calendar**: Schedule meetings and events
- **Document Generation**: Create reports, PRDs, technical specs

### 4. Data & Analytics Tools
- **Database Queries**: Execute read-only database queries
- **Data Visualization**: Generate charts and graphs
- **Statistical Analysis**: Perform calculations and modeling
- **File Processing**: Parse CSV, JSON, XML files

## Security & Safety Framework

### Permission System
- **Role-Based Access**: Tools assigned based on agent persona
- **Approval Workflows**: High-risk tools require human approval
- **Sandboxing**: Isolated execution environments for code/scripts
- **Rate Limiting**: Per-agent and per-tool usage limits
- **Audit Logging**: Complete trail of all tool usage

### Safety Measures
- **Input Validation**: Strict parameter validation before execution
- **Output Sanitization**: Clean and validate tool outputs
- **Resource Limits**: CPU, memory, and time constraints
- **Network Isolation**: Controlled external network access
- **Rollback Capabilities**: Undo mechanisms for destructive operations

## Implementation Architecture

### Tool Registry Service
- Centralized tool definition storage
- Dynamic tool loading and registration
- Version management for tool definitions
- Dependency resolution and validation

### Execution Engine
- Asynchronous tool execution with queuing
- Parallel execution support for independent tools
- Circuit breaker patterns for failing tools
- Retry logic with exponential backoff

### Monitoring & Observability
- Real-time execution dashboards
- Cost tracking and budget alerts
- Performance metrics and optimization insights
- Error analysis and debugging tools

## Potential Pitfalls

- **Tool Selection Confusion**: Agents choosing inappropriate tools or failing to recognize when tools are needed
- **Parameter Hallucination**: LLMs generating invalid or dangerous parameters for tool calls
- **Security Vulnerabilities**: Insufficient sandboxing or validation leading to system compromise
- **Cost Explosion**: Uncontrolled tool usage leading to unexpected API or compute costs
- **Latency Issues**: Tool execution blocking agent responses and degrading user experience
- **Error Cascade**: Tool failures causing agent confusion and poor recovery
- **Permission Complexity**: Overly complex permission systems hindering legitimate use cases

## Good Practices

- **Progressive Enhancement**: Start with safe, read-only tools before adding write operations
- **Clear Tool Documentation**: Comprehensive descriptions with examples and edge cases
- **Graceful Degradation**: Agents should handle tool failures elegantly and continue discussions
- **User Transparency**: Clear indication of tool usage and results in the UI
- **Incremental Rollout**: Deploy tools gradually with monitoring and feedback loops
- **Cost Awareness**: Built-in cost estimation and budget management
- **Testing Framework**: Comprehensive testing for tool definitions and execution paths

## Definition of Done (DoD)

- Tool definition schema and registration system implemented
- At least 5 tools from different categories implemented and tested
- Security framework with sandboxing and permission system operational
- Agent architecture updated to support tool calling and result processing
- Frontend UI components for tool usage visualization and approval workflows
- Comprehensive monitoring and logging for tool execution
- Documentation for tool developers and system administrators
- Integration tests covering tool selection, execution, and error handling
- Performance benchmarks and optimization guidelines established
- Security audit completed for all implemented tools

## End-to-End (E2E) Flows

1. **Agent Autonomous Tool Use:**
   - Agent identifies need for external information during discussion
   - LLM reasoning selects appropriate tool from available toolkit
   - Agent generates tool call with validated parameters
   - Tool execution engine processes request asynchronously
   - Results integrated into agent's response and displayed in UI
   - Tool usage logged for monitoring and cost tracking

2. **Multi-Tool Workflow:**
   - Agent receives complex task requiring multiple tools
   - Agent plans sequence of tool calls with dependencies
   - Tools executed in parallel where possible, sequentially where dependent
   - Intermediate results passed between tools as needed
   - Final synthesized result presented to user with execution summary

3. **Approval-Required Tool Use:**
   - Agent identifies need for high-risk tool (e.g., deployment, external API write)
   - Tool call generated and marked for approval
   - Human moderator receives approval request with context and risk assessment
   - Upon approval, tool executes with full audit trail
   - Results communicated back to agent and discussion participants

4. **Tool Failure Recovery:**
   - Agent attempts tool call that fails (API down, invalid parameters, timeout)
   - Execution engine captures error details and categorizes failure type
   - Agent receives structured error information
   - Agent reasoning determines alternative approach or graceful degradation
   - Discussion continues with explanation of limitations and alternative solutions

## Integration Points

- **Epic 1 (Backend)**: Tool execution APIs and permission management endpoints
- **Epic 2 (Knowledge Graph)**: Semantic search and knowledge retrieval tools
- **Epic 3 (Agent Tooling)**: Foundation for this epic's advanced capabilities
- **Epic 4 (Artifacts)**: Tool-generated artifacts and deployment automation
- **Epic 5 (Testing)**: Comprehensive testing framework for tool reliability

## Success Metrics

- Tool usage adoption rate across different agent personas
- Tool execution success rate and error recovery effectiveness
- User satisfaction with agent capabilities and transparency
- Security incident rate and response time
- Cost efficiency and budget adherence
- Performance impact on discussion flow and responsiveness 